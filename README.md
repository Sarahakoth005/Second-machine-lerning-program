# Second-machine-learning-program
Use of random forest model for prediction

The (random forest) algorithm establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or mean of the output from various trees. Increasing the number of trees increases the precision of the outcome.

A random forest eradicates the limitations of a decision tree algorithm. It reduces the overfitting of datasets and increases precision. It generates predictions without requiring many configurations in packages

# CHARACTERISTICS OF RANDOM FOREST

It’s more accurate than the decision tree algorithm.

It provides an effective way of handling missing data.

It can produce a reasonable prediction without hyper-parameter tuning.

It solves the issue of overfitting in decision trees.

In every random forest tree, a subset of features is selected randomly at the node’s splitting point.
